{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 포스팅 시리즈는 다양한 머신러닝 테크닉에 대해 수학적 관점과 실용적 관점에서 정리한다.\n",
    "\n",
    "필자는 수학을 전공했기 때문에 수학적 접근과 용어에 대해 익숙하게 사용한 것이 있지만, 수학을 전공하지 않은 사람들에겐 다소 낯선 접근과 용어가 있을 수 있다.\n",
    "\n",
    "최대한 그러한 부분을 자세히 설명하려 노력하였지만 필자의 타전공자에 대한 '공감능력부족'으로 효과적으로 전달되지 못한 부분이 있을 것으로 생각된다.\n",
    "\n",
    "이 글을 읽어주시는 분께 일차적으로 감사드리며, 해당 부분에 대해 질문이나 코멘트를 남겨주시는 분께는 거듭제곱으로 감사드림을 말씀드린다.\n",
    "\n",
    "# Support Vector Machine\n",
    "\n",
    "서포트 벡터 머신은 딥러닝이 등장하기 이전에 가장 유명하고 성능 좋은 머신러닝 모델이었다고 한다. \n",
    "현재는 다소 실무에서 사용되는 정도가 줄어들었겠지만, 서포트 벡터 머신에 적용되는 다양한 수학적 테크닉들은 인공지능을 이해하고 연구하는 데에 여전히 훌륭한 인사이트를 준다고 생각한다.\n",
    "특히 서포트 벡터 머신의 아이디어는 유클리드 기하학과 최적화 이론으로 설명이 된다는 점은 수학자들에게 있어서 굉장히 매력적이다.\n",
    "\n",
    "본 포스팅의 내용은 다음의 자료들을 참고했다.\n",
    "\n",
    "> 1. Mathematics for Machine Learning (Deisenroth, Marc Peter and Faisal, A. Aldo and Ong, Cheng Soon)\n",
    "> 2. The Elements for Statistical Learning (Trevor Hastie, Robert Tibshirani, Jerome Friedman)\n",
    "> 3. 김민준님(이화여자대학교, 수학과 석사)의 SVM Lecture note\n",
    "> 4. 김원화 교수님(포항공과대학교, 인공지능대학원 교수)의 데이터 마이닝 Lecture note\n",
    "> 5. Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow (Aurelien, Geron)\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "**Support Vector Machine**(SVM)은 데이터를 분류하기 위해 클래스 사이의 마진(margin)을 최대화하는 초평면을 찾는다. \n",
    "\n",
    "예를들어, 두 개의 특성변수를 가지는 이진분류 문제는 데이터를 2차원 평면상에 표현할 수 있으며, SVM은 이들 두 클래스를 분할하면서 마진(직선 양 옆으로 평행하게 뻗은 띠)을 최대화하는 직선을 찾는다.\n",
    "\n",
    "> **용어정리**\n",
    "> \n",
    "> - *서포트 벡터 (Support Vector)* : 새로운 데이터가 들어왔을 때, 해당 데이터를 구분시켜줄 기준이 되는 샘플\n",
    "> - *결정 경계 (Decision Boundary)* : 모델에 의해 만들어진 클래스를 구분할 기준이 되는 초평면\n",
    "> - *마진 (Margin)* : 결정 경계를 중심으로 서포트 벡터까지의 거리만큼 나란하게 떨어진 영역\n",
    "\n",
    "![svm_figure.png](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/margin.png?raw=true)\n",
    "\n",
    "SVM은 확률모델에 기반으로한 머신러닝 모델들과는 다른 두 가지 특징을 가진다.\n",
    "\n",
    "> 1. SVM은 지도학습 문제를 기하학적인 방법으로 접근한다. 이는 확률모델의 관점으로 접근하는 많은 머신러닝 모델들과 구별되는 점이다.\n",
    "> 2. (soft-margin) SVM의 최적화 문제는 해석적 해를 구하기 어렵다. 따라서 다양한 최적화 방법론(e.g. 경사하강법)에 의존해야 한다.\n",
    "\n",
    "확률모델에 기반한 머신러닝 모델은 *최대우도추정*(Maximum Likelihood Estimation)과 *베이지안 추정*(Bayesian inference)를 취하고 있다.\n",
    "이러한 관점에서 확률기반 모델은 데이터의 유사성에 대한 확률론적 시각(e.g. 확률분포)을 기반으로한 최대우도를 추정하는 최적화문제로 모델추론에 접근한다.\n",
    "\n",
    "그러나 SVM은 데이터의 유사성에 대한 기하학적 시각을 기반으로 최적화문제를 제시한다.\n",
    "데이터의 유사성에 대한 기하학적 시각은 *내적*(inner product)과 *거리*(metric) 개념에 의존한다.\n",
    "SVM은 두 클래스 간의 거리(margin)를 가장 크게 만드는 최적화문제로 모델추론에 접근한다.\n",
    "\n",
    "SVM은 훌륭한 모델이지만, 현재는 딥러닝 모델이 훨씬 범용적으로 사용되고 좋은 성능을 보여준다.\n",
    "이러한 점에서 필자는 단순히 SVM을 얕게 이해하고 모델을 구현하는 방법을 익히는 것이 그렇게 큰 도움이 될 것이라고 생각하지 않는다.\n",
    "그러나 SVM을 공부하는 것은 인공지능 모델을 설계하는 데에 다양한 기하학적 직관을 제공한다는 점에서 공부할 가치가 있다고 생각한다.\n",
    "SVM 이론에 포함된 기하학적 인사이트, 선형분류의 한계를 뛰어넘기 위한 다양한 전략들은 수학적으로 잘 formulation 되어 있기 때문에 수학적으로 훈련된 이들에겐 SVM만큼 다양한 인사이트를 제공하는 모델이 없다고 생각한다.\n",
    "본 포스팅을 작성하는 필자나 읽는 독자나 이 시리즈가 마쳤을 때에 자신만의 훌륭한 인사이트를 가지고 떠나길 바라는 바이다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Binary Classification\n",
    "\n",
    "이진분류 문제는 데이터마다 할당된 출력변수 $y$의 값이 두 가지인 경우를 말한다.\n",
    "\n",
    "예를들어, 소비자들의 다양한 정보를 입력받아 이 소비자가 특정 상품을 구매 할지 안할지 예측하는 문제나, 신호에 대한 정보를 입력받아 이 정보가 진짜 신호인지 가짜 신호인지 분별하는 문제가 이진분류 문제이다.\n",
    "\n",
    "이진분류 문제에서 출력변수는 0 또는 1로 표기하거나 -1 또는 +1로 표기한다.\n",
    "Support Vector Machine을 설명할 때엔 이진분류 문제의 출력변수를 주로 -1과 +1로 표현한다.\n",
    "\n",
    "이진분류 문제를 수학적으로 표현해보자.\n",
    "\n",
    "$N$개의 데이터 $(x_1,y_1), (x_2,y_2), \\ldots , (x_N, y_N)$으로 구성된 훈련 데이터가 주어졌다고하자.\n",
    "여기서 $x_i \\in \\mathbb{R}^d$ 이고, $y \\in \\left\\{-1, +1\\right\\}$이다. \n",
    "\n",
    "지도학습의 관점에서 이진분류 문제는 훈련 데이터를 학습하여 분류 오차를 최소가 되게 하는 분류함수 $G : \\mathbb{R}^d \\rightarrow \\left\\{-1, +1 \\right\\}$를 추정하는 문제이다.\n",
    "\n",
    "선형분류 문제는 훈련 데이터로부터 적절한 선형함수 $f(x) = x^T \\beta + \\beta_0$를 적합하여, 이로부터 정의된 초평면 $\\left\\{ x \\in \\mathbb{R}^d : f(x) = x^T \\beta + \\beta_0 = 0 \\right\\}$을 찾는 문제다.\n",
    "여기서 $\\beta_0 \\in \\mathbb{R}$은 선형함수 $f$의 *절편*(intercept)이며, 파라미터 $\\beta \\in \\mathbb{R}^d$ ($\\lVert \\beta \\rVert = 1$)는 초평면에 수직인 단위벡터, 즉 *법선벡터*(normal vector)이다. \n",
    "\n",
    "분류함수 $G$는 $f(x)$에 의해 $$G(x) = sign[f(x)] = sign[x^T\\beta + \\beta_0]$$ 로 유도된다.\n",
    "여기서 $sign(x) = \\begin{cases} +1 & (x \\geq 0) \\\\ -1 & (x < 0)\\end{cases}$는 *부호함수*이다.\n",
    "그리고 초평면 $\\left\\{ x \\in \\mathbb{R}^d : f(x) = x^T \\beta + \\beta_0 = 0 \\right\\}$를 **결정경계**(Decision boundary)라고 부른다.\n",
    "\n",
    "선형함수 $f(x)$의 파라미터 $\\beta$는 결정경계를 기준으로 방향을 결정한다.\n",
    "테스트 데이터 $x_{test}$가 $f(x_{test}) \\geq 0$이면 결정경계의 *양의 방향*에 놓인다고 하며, 이 경우 $G(x_{test}) = +1$이다.\n",
    "유사하게, $f(x_{test}) < 0$이면 데이터가 결정경계의 *음의 방향*에 놓인다고 하며, 이 경우 $G(x_{test}) = -1$이다.\n",
    "\n",
    "훈련 데이터 $x_i$가 $y_i = +1$의 레이블을 가진다면, $f(x_i) \\geq 0$이 되어야 $G(x_i) = +1$이 된다.\n",
    "마찬가지로 $y_i = -1$의 레이블을 가진다면, $f(x_i) < 0$이 되어야 $G(x_i) = -1$이 되어 정확한 예측이 된다.\n",
    "**선형분류 문제**는 모든 $i$에 대하여 $y_if(x_i) > 0$이 되게 하는 선형함수 $f(x) = x^T \\beta + \\beta_0$를 찾을 수 있는 문제를 말한다.\n",
    "\n",
    "![linear_classification.png](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/linear_classification.png?raw=true)\n",
    " \n",
    "이때 두 클래스를 분류할 수 있는 선형함수 $f(x)$의 선택, 즉 결정경계는 여러개 존재할 수 있다.\n",
    "그렇다면 어떤 결정경계를 선택하는 것이 가장 일반화 성능이 좋다고 할 수 있을까?\n",
    "\n",
    "SVM은 이 질문에 대해 유클리드 기하학과 최적화 이론의 언어로 답을 한다.\n",
    "모델의 융통성을 얻기 위해 SVM은 **마진**(margin)이 최대가 되도록 선형함수 $f(x)$를 선택한다.\n",
    "마진은 유클리드 기하학의 개념으로 결정경계의 근방영역을 정의하며, 이 영역이 넓을 수록 결정경계 근방에 위치하게 될 테스트 데이터에 대해 유연하게 대처할 수 있게 된다.\n",
    "이러한 마진 최대화 문제는 수학적으로 최적화 이론의 언어로 잘 정의되며, 이를 알고리즘으로 구할 수 있다.\n",
    "\n",
    "다음 포스팅부터는 이러한 개념의 정의에 대해 차근차근 알아가보도록 하겠다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **연습문제 1)**\n",
    "> 벡터 $\\beta$가 초평면 $\\left\\{x \\in \\mathbb{R}^d : f(x) = x^T \\beta + \\beta_0 = 0\\right\\}$와 수직임을 보여라.\n",
    "> \n",
    "> *Solution)*\n",
    "> \n",
    "> 초평면 위의 임의의 두 점 $x_a$, $x_b$에 대하여, 초평면 위의 위치벡터 $x_a - x_b$를 생각하자. \n",
    "> \n",
    "> 이때 초평면의 정의에 의해 $f(x_a) = 0, f(x_b) = 0$이이다.\n",
    "> \n",
    "> 한편, $f(x_a) - f(x_b) = (x_a^T\\beta + \\beta_0) - (x_b^T\\beta + \\beta_0) = (x_a - x_b)^T\\beta$가 성립하므로 $(x_a - x_b)^T\\beta = 0$임을 얻는다. \n",
    "> \n",
    "> 따라서 $\\beta$는 초평면 위의 임의의 위치벡터에 대하여 수직이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hard Margin SVM (1)\n",
    "\n",
    "**마진**(margin)은 결정경계와 그것과 가장 가까이 있는 훈련 데이터와의 거리를 말한다.\n",
    "마진은 학창시절에 배운 *점과 직선 사이의 거리공식*을 일반화한 개념으로 계산한다.\n",
    "\n",
    "### 3.1 점과 직선 사이의 거리공식\n",
    "먼저 점과 직선 사이의 거리 공식을 복습해보자.\n",
    "중고등학교 시절 점 $(x_0,y_0)$와 직선 $ax+by+c = 0$ 사이의 거리 $d$는 아래의 공식으로 구했었다.\n",
    "$$d = \\frac{ax_0 + by_0 + c}{\\sqrt{a^2 + b^2}} $$\n",
    "이 공식은 보통 고등학교 1학년 과정에서 배우게 되는데 고등학교 1학년때는 벡터의 개념을 배우지 않기 때문에 그 유도과정이 상당히 작위적이다. \n",
    "여기서는 벡터를 이용해 위 공식을 다시 유도해보고자 한다.\n",
    "\n",
    "먼저 직선이 어떻게 정의되는지 살펴보자.\n",
    "직선의 방정식이라고 말하면 보통 일차함수 $y = ax + b$ 꼴을 생각한다. \n",
    "이 일차함수에서 $x$의 계수 $a$는 직선의 기울기를, $b$는 $y$-절편을 의미했다.\n",
    "\n",
    "그런데 유달리 점과 직선 사이의 거리 공식을 사용할 땐, 직선을 일차방정식 $ax + by + c = 0$의 꼴로 정의했다.\n",
    "아직 내적의 개념을 배우지 않은 고등학교 1학년에게 이러한 형태의 직선의 방정식에서 $x$와 $y$ 그리고 상수항이 가진 계수 $a,b,c$의 의미를 해석하기가 어려웠다.\n",
    "이 포스팅을 읽고있는 독자들 중에서도 혹시 이 공식의 해석에 대해 잘 몰랐던 분이 계시다면, 이 기회에 배워가보시길 바란다.\n",
    "\n",
    "![<img src = 'distance.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/distance.png?raw=true)\n",
    "\n",
    "\n",
    "먼저 원점을 지나는 직선 $ax + by = 0$을 생각해보자.\n",
    "이 직선 위의 점은 방정식 $ax + by = 0$을 만족시키는 벡터 $[x,y]^T \\in \\mathbb{R}^2$들의 집합으로 표현된다.\n",
    "그런데 $x,y$를 벡터로 본 순간, 식 $ax + by$는 벡터의 내적 공식으로 보인다!\n",
    "즉, 방정식 $ax + by = 0$은 사실 **두 벡터 $[a,b]^T \\in \\mathbb{R}^2$과 $[x,y]^T \\in \\mathbb{R}^2$의 내적 $\\langle [a,b]^T, [x,y]^T \\rangle = [a,b]\\begin{bmatrix} x \\\\ y \\end{bmatrix}$의 값이 0이 된다**는 의미다.\n",
    "내적이 0이 된다는 의미는 **두 벡터가 수직**임을 의미한다.\n",
    "따라서 직선 $ax+by=0$은 벡터 $[a,b]^T$와 수직인 벡터들의 자취를 의미하게 된다.\n",
    "이런 점에서 직선의 방정식 $ax + by = 0$의 계수벡터 $[a,b]^T$는 직선의 진행방향에 수직인 방향을 나타내고, 이를 **법선벡터**(normal vector)라고 부른다.\n",
    "\n",
    "이제 좌표공간 위의 점 $(x_0, y_0)$와 원점을 지나는 직선 $ax + by = 0$ 사이의 거리를 구해보자.\n",
    "점 $(x,y)$를 점 $(x_0, y_0)$의 직선 $ax + by = 0$ 위로의 정사영이라고 하자. \n",
    "그런데 앞에서 법선벡터 $[a,b]^T$는 직선 $ax+by = 0$과 수직이라고 했으므로, $[a,b]^T$와 $[x_0 - x, y_0 - y]^T$는 평행하다.\n",
    "즉, 어떤 상수 $\\alpha \\in \\mathbb{R}$가 존재하여, \n",
    "$$[x_0 - x, y_0 - y]^T = \\alpha [a,b]^T$$\n",
    "가 성립한다.\n",
    "이때 법선벡터 $[a,b]^T$의 크기를 1로 정규화해 방향에 대한 정보만 남겨주면, $(x_0,y_0)$는 $(x,y)$를 $\\frac{1}{\\sqrt{a^2 + b^2}}\\begin{bmatrix} a \\\\ b \\end{bmatrix}$ 방향으로 거리 $d$ 만큼 평행이동한 것이므로, \n",
    "$$\\alpha = \\frac{d}{\\sqrt{a^2 + b^2}}$$\n",
    "이고\n",
    "$$ \\begin{bmatrix} x_0-x \\\\ y_0-y \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\frac{d}{\\sqrt{a^2 + b^2}}\\begin{bmatrix} a \\\\ b \\end{bmatrix}$$\n",
    "임을 유도할 수 있다.\n",
    "\n",
    "그런데 $(x_0,y_0)$와 $(x,y)$ 사이의 거리 $d$는 \n",
    "$$d = \\sqrt{[x_0-x,y_0-y]\\begin{bmatrix} x_0-x \\\\ y_0-y \\end{bmatrix}}$$ \n",
    "이 성립하므로\n",
    "$$ d^2 = [x_0-x,y_0-y]\\begin{bmatrix} x_0-x \\\\ y_0-y \\end{bmatrix} = [x_0-x,y_0-y]\\left(\\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\frac{d}{\\sqrt{a^2 + b^2}}\\begin{bmatrix} a \\\\ b \\end{bmatrix}\\right)$$\n",
    "가 성립한다.\n",
    "\n",
    "이때 $[x_0-x,y_0-y]\\begin{bmatrix} x \\\\ y \\end{bmatrix} = 0$이고 $ax + by =0$ 이므로 위 식을 정리하면, \n",
    "$$ d^2 = \\frac{d}{\\sqrt{a^2 + b^2}}\\begin{bmatrix} x_0-x \\\\ y_0-y \\end{bmatrix}^T\\begin{bmatrix} a \\\\ b \\end{bmatrix} = d\\frac{ax_0 + by_0}{\\sqrt{a^2 + b^2}}$$\n",
    "이 되어, \n",
    "$$d = \\frac{ax_0 + by_0}{\\sqrt{a^2 + b^2}}$$\n",
    "임을 얻는다.\n",
    "\n",
    "중간에 몇 가지 계산과정을 생략했는데 손으로 계산해보면서 생략된 부분을 직접 매꾸어 보면 이 유도과정을 이해하는 데에 도움이 될 것이다.\n",
    "\n",
    "(원점을 지나지 않는 직선의 케이스는 평행이동의 개념으로 옮겨서 이해한다. 추후 추가 예정)\n",
    "\n",
    "\n",
    "### 3.2 마진(margin)\n",
    "이제 마진을 정의하기 위해 필요한 준비는 모두 끝났다.\n",
    "$x_a \\in \\mathbb{R}^d$가 결정경계와 가장 가까운 훈련 데이터라고 하고, 결정경계는 선형함수 $f(x) = x^T\\beta + \\beta_0$에 의해 정의된다고 하자.\n",
    "이때 선형함수의 법선벡터 $\\beta$에 대하여, $\\lVert \\beta \\rVert =  1$이라 가정한다.\n",
    "$x_a$와 결정경계 사이의 최단거리는 $x_a$의 결정경계 위로의 사영 $x_a'$와 $x_a$ 사이의 거리 $d$로 정의된다.\n",
    "그런데 결정경계와 단위법선벡터 $\\beta$가 수직이므로, 벡터 $x_a$는 $x_a' + d\\beta$로 표현될 수 있다.\n",
    "\n",
    "\n",
    "![<img src = 'margin.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/margin.png?raw=true)\n",
    "\n",
    "위와 같은 개념을 생각하면, 결정경계의 양의 방향과 음의 방향으로 $d$ 만큼 떨어진 영역을 생각할 수 있다.\n",
    "이 영역 또는 이 영역의 폭을 **마진**(margin)이라고 부른다.\n",
    "마진을 이용하면 모든 $i$에 대하여, $y_i f(x_i) \\geq d$가 되도록 하는 선형함수 $f(x)$를 찾는 문제로 선형분류 문제를 변형할 수 있다.\n",
    "\n",
    "\n",
    "![<img src = 'margin_ex.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/margin_ex.png?raw=true)\n",
    "\n",
    "\n",
    "마진을 최대화 하는 선형함수 $f(x)$를 찾는 문제는 다음의 최적화문제로 표현할 수 있다.\n",
    "$$ \\max_{\\beta, \\beta_0, d} d \\\\ \\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq d, d>0,  \\forall i=1,2,\\ldots, N$$\n",
    "SVM은 위 최적화문제의 해로서 결정경계를 선택한다.\n",
    "즉, 마진의 경계 외부에서 클래스가 분류되도록 하면서 마진의 크기를 최대화 시키는 문제다.\n",
    "\n",
    "마진 최대화 문제를 이렇게 해석하면 직관적으로 문제를 이해하기가 편리하다.\n",
    "그러나 이 최적화 문제를 푸는 데에는 다소 유리하지 않다.\n",
    "\n",
    "다음 포스팅에서는 이 최적화 문제를 수학적으로 풀기 쉬운 형태로 유도하는 방법에 대해서 다루도록 하겠다.\n",
    "\n",
    "\n",
    "> **주의!**\n",
    ">\n",
    "> 마진의 개념은 유클리드 공간 상의 거리 개념으로부터 유도된다.\n",
    "> 그러나 거리 개념은 데이터의 스케일에 따라 혼란을 야기할 수 있다.\n",
    ">\n",
    ">예를들어, 입력변수 $x_i$가 '계좌잔액', '개설연도'로 구성된 데이터라고 할 때 $x_a = (1000000, 2020)$, $x_b = (10000, 2020)$, $x_c = (1000020, 2000)$ 세 가지 샘플의 거리는 어떻게 이해할 것인가?\n",
    ">$x_a$와 $x_b$는 개설연도가 같지만, 계좌잔고가 100배나 차이나므로 거리가 굉장히 크다.\n",
    ">$x_a$와 $x_c$는 계좌잔고가 20원밖에 차이가 나지 않지만, 개설연도가 20년이나 차이가 난다.\n",
    ">$x_b$와 $x_c$ 중에 $x_a$와 더 유사한 데이터는 어느 것인가?\n",
    ">이는 문제에 따라 다를 것이다. \n",
    ">그러나 단순히 Euclidean 거리로 계산했을 때엔 $x_a$와 $x_b$ 사이의 거리가 $x_a$와 $x_c$ 사이의 거리보다 매우 크다.\n",
    ">\n",
    ">이러한 점 때문에 거리 개념을 기반으로한 SVM 모델은 데이터의 scale에 민감하다고 할 수 있다.\n",
    ">따라서 SVM 모델을 학습하기 전에 스케일링을 위한 전처리기법(e.g. Standard Scaling)을 수행하는 것이 일반적이다.\n",
    ">데이터의 스케일링에 대하여 더 자세한 논의는 후에 하도록 하고 지금은 데이터의 스케일링 이슈는 고려하지 않도록 하자.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hard Margin SVM (2)\n",
    "\n",
    "\n",
    "[이전 포스팅](https://velog.io/@shlee0125/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%A0%95%EB%A6%AC-Support-Vector-Machine-03.-Hard-Margin-SVM-1)에서는 선형함수 $f(x)$의 방향만 고려하기 위해 제약조건으로 $\\lVert \\beta \\rVert = 1$이 되게 두고 마진 최대화 문제를 유도했다.\n",
    "만약 선형함수 $f(x)$의 법선벡터 $\\beta$가 단위벡터가 아니더라도, 그 크기로 정규화 $(\\beta / \\lVert \\beta \\rVert)$해주는 것으로 동일한 문제를 유도할 수 있다.\n",
    "\n",
    "이번에는 파라미터 벡터 $\\beta$를 정규화하는것 대신 단위 또는 기준이 될 데이터를 선택하는 것으로 동일한 문제를 유도해보겠다.\n",
    "우리가 선텍할 기준은 예측값 $f(x) = x^T \\beta + b$의 값이 1이 되는 데이터를 '가장 가까운 데이터'라고 생각하는 것이다.\n",
    "이러한 데이터를 앞에서와 마찬가지로 $x_a$로 표기하자.\n",
    "앞에서는 $x_a$에 대해 특별한 가정 없이 '가장 가까운 데이터'로 선택했지만, 이번에는 '가장 가깝다'의 기준을 선형함수 $f(x)$의 값을 기준으로 설정한다는 점에서 차이가 있음에 유의하자.\n",
    "\n",
    "그러면 $x_a$는 $f(x_a) = 1$을 만족시키므로, 초평면 $\\left\\{x \\in \\mathbb{R}^d : f(x) = 1 \\right\\}$ 위에 놓이게 된다.\n",
    "이번에도 마찬가지로 $x_a$의 결정경계 위로의 정사영 $x_a'$를 생각한다.\n",
    "그러면 $x_a'$는 방정식 \n",
    "$$f(x_a') = x_a'^T \\beta + \\beta_0 =  0 \\;\\; \\cdots (1)$$\n",
    "을 만족시킨다.\n",
    "이때 $\\beta$가 앞에서와 달리 단위벡터일 필요가 없음에 유의하자.\n",
    "한편, 결정경계와 $x_a$ 사이의 거리를 $d$라고 두면 아래의 관계식이 성립한다.\n",
    "$$ x_a' = x_a - d\\frac{\\beta}{\\lVert \\beta \\rVert} \\;\\; \\cdots (2)$$\n",
    "이제 이 식을 위 방정식 (1)에 대입하여 정리하면, 다음 관계식을 얻는다. (유도과정은 연습문제로 남겨둔다.)\n",
    "$$ d = \\frac{1}{\\lVert \\beta \\rVert} \\cdots (3)$$\n",
    "\n",
    "![<img src = 'margin2.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/margin2.png?raw=true)\n",
    "\n",
    "$d$를 이러한 방식으로 유도해주면 모든 $i$에 대하여, $y_i f(x_i) \\geq 1$가 되도록 하는 선형함수 $f(x)$를 찾는 문제로 선형분류 문제를 변형할 수 있다.\n",
    "그러므로 마진 최대화 문제는 다음의 최적화 문제로 정의될 수 있다.\n",
    "$$ \\max_{\\beta, \\beta_0} \\frac{1}{\\lVert \\beta \\rVert} \\\\ \\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq 1,  \\forall i=1,2,\\ldots, N$$\n",
    "\n",
    "이 문제를 계산의 편의를 위해 $\\frac{1}{2} \\lVert \\beta \\rVert^2$를 최소화 하는 문제로 변형하자.\n",
    "이렇게 변형해도 최적해는 변함이 없지만, 해를 구하는 계산과정에서 미분 등의 연산을 할 때 훨씬 편리해진다.\n",
    "이를 통해 최종적으로 다음의 최적화 문제를 얻는다.\n",
    "\n",
    "$$ \\min_{\\beta, \\beta_0} \\frac{1}{2}\\lVert \\beta \\rVert^2 \\\\ \\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq 1,  \\forall i=1,2,\\ldots, N$$\n",
    "\n",
    "위 최적화 문제는 **하드 마진 SVM**(Hard margin SVM)이라 부른다.\n",
    "이 문제를 '하드'하다고 부르는 이유는 이 문제의 유도에서 마진 조건 $y_i(x_i^T\\beta + \\beta_0) \\geq 1$에 조금의 오차도 허용하지 않기 때문이다.\n",
    "이 문제는 선형 부등식 제약조건을 가지는 컨벡스 최적화 문제로, 라그랑주 승수법을 이용해 해석적으로 풀기에 용이하다.\n",
    "\n",
    "최소화 형태의 하드 마진 SVM 최적화 문제와 지난 포스팅에서 정의한 최대화 형태의 하드 마진 SVM 최적화 문제는 동치다.\n",
    "두 문제가 동치임을 보이는 것은 연습문제로 남겨둔다.\n",
    "\n",
    "이 문제의 해석적 해법에 대해서는 다음에 살펴보도록 하자.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **연습문제 2)**\n",
    "> 방정식 (1)에 방정식 (2)를 대입하여 방정식 (3)을 유도하시오.\n",
    "> \n",
    "> *Solution)*\n",
    ">\n",
    "> 방정식 (1)에 방정식 (2)를 대입하면 다음을 얻는다. \n",
    "> $$ \\left(x_a - d\\frac{\\beta}{\\lVert \\beta \\rVert}\\right)^T \\beta + \\beta_0 = 0 $$\n",
    "> 그러면 전치연산자 $T$의 성질에 의해, \n",
    "> $$ \\left(x_a^T - d\\frac{\\beta^T}{\\lVert \\beta \\rVert}\\right) \\beta + \\beta_0 = 0 $$\n",
    "> 임을 얻는다.\n",
    "> 그러면 행렬곱의 분배법칙에 의해\n",
    "> $$ \\left(x_a^T \\beta - d \\frac{\\beta^T\\beta}{\\lVert\\beta \\rVert}\\right) + \\beta_0 = 0$$\n",
    "> 이 성립한다.\n",
    "> 이제 위 식을 $$(x_a^T\\beta + \\beta_0) - d\\frac{\\beta^T\\beta}{\\lVert\\beta \\rVert} = 0$$\n",
    "> 으로 정리하자.\n",
    ">\n",
    "> 여기서 $x_a$는 $f(x_a) = 1$이 만족하도록 선택했으므로, $x_a^T\\beta + \\beta_0 = 1$이고 내적의 성질에 의해 $\\beta^T \\beta = \\lVert \\beta \\rVert^2$ 임을 이용하면,\n",
    "> $$ 1 - d\\lVert \\beta \\rVert = 0$$\n",
    "> 임을 얻는다.\n",
    "> 따라서 이를 정리하면, \n",
    "> $$d = \\frac{1}{\\lVert \\beta \\rVert} \\;\\; \\cdots (3)$$\n",
    "> 를 얻는다.\n",
    "\n",
    "---\n",
    "\n",
    "> **연습문제 3)**\n",
    "> 마진 최대화 문제 \n",
    "> $$ \\max_{\\beta, \\beta_0, d} d \\\\ \\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq d, d>0,  \\forall i=1,2,\\ldots, N$$\n",
    "> 와\n",
    "> 파라미터 최소화 문제 \n",
    "> $$ \\min_{\\beta, \\beta_0} \\frac{1}{2}\\lVert \\beta \\rVert^2 \\\\ \\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq 1,  \\forall i=1,2,\\ldots, N$$\n",
    "> 가 동치임을 보이시오.\n",
    "> \n",
    "> \n",
    "> *Solution)*\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Why does SVM maximize margin?\n",
    "\n",
    "지난 포스팅까지 SVM이 마진을 최대화 하도록 선형결정경계를 만드는 알고리즘임을 살펴보았다.\n",
    "그렇다면 마진을 최대화 하면 좋은 점이 무엇일까?\n",
    "이번 포스팅에서는 마진 최대화의 이점에 대해 알아보자.\n",
    "\n",
    "### 5.1 신용과 의심\n",
    "이진분류 문제는 의사결정 문제라고 불리기도 한다.\n",
    "우리가 어떤 의사를 할지 말지 결정할 때는 가지고 있는 정보를 바탕으로 **신용**과 **의심**의 줄타기(Trade-off)를 해야한다.\n",
    "\n",
    "가지고 있는 정보를 완벽한 정보라고 생각하고 너무 믿어버리면 편향된 선택을 할 가능성이 높아진다.\n",
    "그렇다고 가지고 있는 정보를 너무 의심하면 아무런 결정도 내릴 수 없다.\n",
    "그러므로 현재의 정보를 **최대한 믿으면서** 동시에 **의심의 여지도 최대한** 마련해 두는 것이 적절한 의사결정 기준일 것이다.\n",
    "\n",
    "![<img src = 'confidence.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/confidence.png?raw=true)\n",
    "\n",
    "\n",
    "### 5.2 거리로 해석한 신용도\n",
    "SVM은 위와 같은 생각을 기하학적으로 해석해 풀어낸 것이다.\n",
    "**결정경계에서 샘플까지의 거리**를 의사결정에 대한 **신용도**라고 해석하는 것이다.\n",
    "(신뢰도라는 단어가 의미상 더 적절하겠으나, 통계학적 관점의 신뢰도 개념와 구별하기 위해 신용도라는 단어를 선택했다.)\n",
    "\n",
    "현재의 데이터를 최대한 믿는다는 것은 현재의 데이터가 올바로 분류되어 있을 것으로 생각하고 이를 완벽히 분류하는 결정경계를 선택한다는 것을 의미한다.\n",
    "**의심의 여지**를 최대한 마련해 둔다는 것은 현재 의사결정 기준이 아직 모르는 데이터에 대해서는 완벽하지 않을 수 있음을 인정하고 현 상황에서 마진이 최대가 되는 기준을 택한다는 것이다.\n",
    "\n",
    "\n",
    "![<img src = 'confidence2.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/confidence2.png?raw=true)\n",
    "\n",
    "\n",
    "마진을 결정하게 되는 각 클래스의 샘플을 **서포트 벡터**라고 부른다.\n",
    "훈련 데이터에서 서포트 벡터를 어떤 것으로 선택하느냐에 따라 결정경계가 변하게 되므로, 서포트 벡터는 **새로운 데이터가 들어왔을 때 클래스를 구분하는 기준이 되는 샘플**이라고 할 수 있다.\n",
    "\n",
    "\n",
    "서포트 벡터를 선택하는 기준은 일차적으로 **훈련 데이터를 완벽히 분류할 수 있도록** 선택한다.\n",
    "현재 *선형*분류 문제를 풀고 있는 상황을 가정하고 있음을 명심하자.\n",
    "그러므로 이러한 서포트 벡터는 항상 존재하며 선택할 수 있을 것이다.\n",
    "이는 훈련 데이터가 앞으로의 데이터를 예측 하는데에 유용할 것이라는 **신용**을 바탕으로 한다.\n",
    "\n",
    "서포트 벡터를 선택하는 다음 기준은 **서포트 벡터를 최대한 의심하는 방향으로** 선택하는 것이다.\n",
    "앞으로의 데이터는 어디서 나타날지 알 수 없다.\n",
    "그러므로 마진이 최대가 되는 결정경계를 택하면 현재 데이터로 설명할 수 없는 데이터를 만나더라도 올바르게 분류할 가능성이 높을 것이다.\n",
    "이는 훈련 데이터만을 너무 믿어서는 안된다는 **의심**을 바탕으로 한다.\n",
    "\n",
    "\n",
    "### 5.3 결정경계 근방에서의 섭동\n",
    "아래의 두 결정경계를 비교해보자.\n",
    "\n",
    "![margin_comparison.png](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/margin_comparision.png?raw=true)\n",
    "\n",
    "\n",
    "노란색 결정경계는 마진이 좁고 초록색 결정경계는 마진이 넓다.\n",
    "지금까지의 논의에 따르면 서포트 벡터는 훈련 데이터 중에서 **가장 의심가는 샘플**이라고 할 수 있다.\n",
    "어떤 결정경계더라도 신용도가 높은 샘플은 문제가 되지 않는다.\n",
    "문제는 결정경계 근방에 있는 데이터에서 발생한다.\n",
    "\n",
    "노란색 결정경계의 경우 서포트 벡터의 미동(Perturbation)에 대해 민감하게 판단이 바뀐다. (노란색 원)\n",
    "따라서 노란색 결정경계를 신뢰한다면, 결정경계 근방의 테스트 데이터에 대해서 결정오차가 커질 가능성이 높아진다고 추측할 수 있다.\n",
    "\n",
    "그러나 초록색 결정경계의 경우 서포트 벡터의 미동에 대해서도 판단이 민감하지 않다. (초록색 원)\n",
    "따라서 초록색 결정경계를 신뢰한다면, 결정경계 근방의 테스트 데이터에 대해서 결정오차가 커질 가능성이 상대적으로 낮아진다고 추측할 수 있다.\n",
    "\n",
    "이런 점에서 마진이 넓은 결정경계가 더 유용할 것이라 생각할 수 있다.\n",
    "\n",
    "### 5.4 오버피팅의 위험\n",
    "\n",
    "머신러닝의 관점으로 설명하자면 마진이 좁은 결정경계는 훈련 데이터에 대한 **오버피팅** 문제를 야기한다고 할 수 있다.\n",
    "두 결정경계 모두 훈련 데이터에 대해서는 높은 정확도를 보여주지만, 노란색 결정경계는 훈련 데이터를 **신용하는 정도가 과도하게 커** 앞으로 나타날 데이터에 대해서는 다소 낮은 정확도를 보여줄 수 있다.\n",
    "\n",
    "따라서 SVM은 마진을 최대화하는 것으로 현재 데이터로부터 결정경계를 가능한한 멀리 떨어트려놓음으로써 오버피팅의 위험을 가능한한 최소화하고자 했다고 볼 수 있다.\n",
    "이것이 SVM 모델이 우수한 성능을 낼 수 있었던 이유 중 하나일 것이다.\n",
    "\n",
    "---\n",
    "\n",
    "지금까지 SVM이 왜 마진을 최대화하고자 하였는지에 대해 논의해보았다.\n",
    "그러나 지금까지 논의는 **선형분류**가 가능하다는 가정을 해왔다.\n",
    "\n",
    "![<img src = 'soft_margin1.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/soft_margin1.png?raw=true)\n",
    "\n",
    "실제 데이터는 노이즈나 이상치가 있는 경우가 많아 선형분류가 어렵다.\n",
    "이런 경우에 하드마진 SVM은 제대로 작동하기 어렵다.\n",
    "\n",
    "다음 포스팅에서는 이상치나 노이즈가 있는 데이터에 대해서도 SVM이 잘 작동할 수 있도록 교정한 **소프트 마진 SVM**(Soft-margin SVM)에 대해서 다루어보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Soft-margin SVM (1)\n",
    "\n",
    "지금까지 살펴봤던 하드마진 SVM은 현실적으로 좋은 성능을 내기 어렵다.\n",
    "현실의 데이터는 **노이즈나 이상치**가 많고 이런 샘플의 존재는 마진이 굉장히 작아질 수 밖에 없도록 강제하기도 한다.\n",
    "\n",
    "![<img src = 'soft_margin1.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/soft_margin1.png?raw=true)\n",
    "\n",
    "이런 점에서 하드마진 SVM은 이상치에 민감하다고 할 수 있다.\n",
    "\n",
    "더욱 심한 경우는 데이터가 **선형분류가 불가능한 경우**다. \n",
    "\n",
    "![<img src = 'soft_margin2.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/soft_margin2.png?raw=true)\n",
    "\n",
    "위 그림과 같이 파란 샘플의 군집에 속한 단 하나의 빨간 샘플에 의해 선형분류가 불가능해질 수 있다.\n",
    "\n",
    "그리고 불행하게도 이러한 일은 현실에서 매우 흔하게 일어난다.\n",
    "\n",
    "### 6.1 잉여인간도 쓸모가 있다\n",
    "\n",
    "[![잉여인간](https://i.ytimg.com/an_webp/TlcFgP6A2Fs/mqdefault_6s.webp?du=3000&sqp=CPff-JQG&rs=AOn4CLD2xB47SiUJY6dpO2TEr7QCcP--zA)](https://youtu.be/TlcFgP6A2Fs)\n",
    "> 영화 말죽거리 잔혹사에서 잉여인간이라는 용어가 등장했다. \"너, 대학 못가면 뭔 줄 알아? 잉여인간이야, 잉여인간!\"\n",
    "> (영상출처 :유튜브 채널 Ailenschzar)\n",
    "\n",
    "필자가 어린 시절에 인터넷에서는 *잉여* 또는 *잉여인간*이라는 말이 인터넷에 유행했었다. \n",
    "현실에선 하릴없이 배나 긁으면서 하루종일 인터넷 커뮤니티에서 온종일 시간을 떼우는 사람들을 지칭하거나 스스로를 자조하는 용어였다. \n",
    "옛날엔 잉여에 대한 취급이 참 많이 박했다.\n",
    "오직 열심히 최선을 다해 무언가 배우고 생산하는 활동만이 유용하다고 여겨졌다.\n",
    "\n",
    "그러나 요즘에 유튜브와 같은 소셜 네트워크 서비스가 발달하면서 잉여로운 컨텐츠들이 사람들에게 인기를 얻고 있다.\n",
    "요즘 사람들이 자신의 삶에 *잉여*로운 시간들을 얼마나 소중하게 여기는지 생각해보라.\n",
    "워라벨이라는 키워드가 한창 많은 사람들이 열광하는 가치관으로 떠오르더니 이젠 스타벅스가 딸린 건물주가 되어서 딱히 열정 없이 잉여롭게 살지만 부족하지는 않은 삶을 꿈꾸고 있다.\n",
    "우리는 *잉여*가 쓸모있는 시대, *잉여인간*이 되길 꿈꾸는 시대에 살고있다.\n",
    "\n",
    "SVM 이야기를 하다가 왜 갑자기 잉여에 대한 이야기를 하냐면, 이 **잉여**(slack)의 개념이 SVM의 한계를 극복할 강력한 무기가 되기 때문이다. \n",
    "하드 마진 SVM이 마진의 잉여에 대해 조금의 융통성도 허락하지 않았지만, 이번 포스팅에서 학습할 **소프트 마진 SVM**(Soft-margin SVM)은 마진에 잉여를 허가해줌으로써 하드 마진 SVM이 가지고 있던 한계를 극복한다.\n",
    "\n",
    "### 6.2 소프트 마진 SVM의 정의 : 기하학적 관점\n",
    "\n",
    "![<img src = 'soft_margin3.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/soft_margin3.png?raw=true)\n",
    "\n",
    "그림과 같이 훈련 데이터의 클래스가 다소 섞여있다고 하자.\n",
    "소프트 마진 SVM은 결정경계를 여전히 선형경계로 택하되, 마진의 폭 내부에 어느 정도 오차가 들어와도 용인해주도록 융통성을 부여해 일반화 성능의 향상을 기대할 수 있도록 한다.\n",
    "\n",
    "하드 마진 SVM의 경우 훈련 데이터가 선형분류가 가능하다는 가정을 상당히 신뢰하였다.\n",
    "그러나 소프트 마진 SVM은 현실적으로 훈련 데이터가 완전하지 못하며, 이상치나 노이즈가 있을 수 있을 수 있다는 점에 오히려 더 무게를 둔다.\n",
    "그러므로 결정 경계의 근방에서 발생할 노이즈나 이상치 데이터는 조금 틀리도록 여유를 두면서 훨씬 더 많은 정상치 데이터는 최대한 잘 분류할 수 있도록 넓은 마진을 가지는 선형경계를 만든다.\n",
    "\n",
    "핵심 아이디어는 각 훈련 데이터 샘플 $(x_i,y_i)$마다 **잉여 변수**(slack variable) $\\xi_i$를 대응시켜서 샘플이 마진의 폭 안으로 $\\xi_i$만큼 파고드는 상황(심하게는 결정경계 건너편으로 넘어가 오답이 되는 상황)을 용인해주는 것이다.\n",
    "\n",
    "![<img src = 'soft_margin4.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/soft_margin4.png?raw=true)\n",
    "\n",
    "이는 각 샘플 $(x_i,y_i),\\; \\forall i=1,2,\\ldots,N$의 제약조건을 $y_if(x_i) \\geq \\red{1 - \\xi_i}$로 변형하는 것으로 구현된다.\n",
    "\n",
    "아무리 잉여가 좋다고 해도, 구성원 모두가 잉여가 되어버리면 그 사회는 무너질게 뻔하다.\n",
    "매정하지만, 잉여는 선택받은 소수만 누릴 수 있도록 잉여의 총합은 최소가 되도록 해야할 것이다.\n",
    "소프트 마진 SVM은 이러한 생각을 반영하여 다음과 같이 마진 최적화 문제를 변형한 것이다.\n",
    "\n",
    "$$\\min_{\\beta, \\beta_0, \\xi} \\frac{1}{2} \\lVert \\beta \\rVert^2 + C\\sum_{i=1}^N \\xi_n \\\\ \\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq 1 - \\xi_i, \\xi_i \\geq 0, \\forall i=1,\\ldots,N$$\n",
    "\n",
    "여기서 하이퍼 파라미터 $C>0$는 마진의 폭과 잉여의 총합 간의 trade-off를 조정하는 **규제 파라미터**(regularization parameter) 또는 **페널티**(Penalty)다.\n",
    "만약 $\\xi_i > 1$라면 소프트 마진 SVM은 샘플 $(x_i,y_i)$을 잘못 분류되게 된다.\n",
    "$C$ 값은 모델이 오답을 낼때마다 벌칙을 얼마나 더 강하게 부여할지 결정한다.\n",
    "모델은 $\\frac{1}{2} \\lVert \\beta \\rVert^2 + C\\sum_{i=1}^N \\xi_n$의 값을 최소화하고 싶은데 $C$ 값이 크다면 오답을 낼수록 목적함수를 최소화하기가 더욱 어려워지기 때문에 $C$를 벌칙(페널티)이라 명명한 것은 적절해보인다.\n",
    "페널티에 대해서는 다음에 더 자세히 알아보도록 하자.\n",
    "\n",
    "---\n",
    "\n",
    "이번 포스팅에서는 하드 마진 SVM의 한계를 극복하기 위해 도입된 소프트 마진 SVM을 소개하고, 기하학적 관점에서 잉여 변수를 정의하고 모델을 해석해보았다.\n",
    "\n",
    "다음 포스팅에서는 잉여가 쓸모있다던 필자가 갑자기 말을 바꾸어, 잉여는 결국 손실이라는 관점에서 소프트 마진 SVM을 도입해볼 것이다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Soft-Margin SVM (2)\n",
    "\n",
    "지난 포스팅에서는 하드 마진 SVM의 한계와 이를 극복하기 위해 **잉여**(slack)를 도입해 일반화 성능을 향상시킨 소프트 마진 SVM에 대해 알아보았다.\n",
    "\n",
    "이번 포스팅에선느 필자가 말을 바꾸려고 한다.\n",
    "결국 **잉여는 사회적 손실**이다.\n",
    "'열심히 살지 않아도 괜찮아'라는 말은 극심해진 빈부격차에 대해 계층의 벽을 뛰어넘으려해도 좌절하여 잉여가 되는 사람들의 불만을 누그러뜨리기 위해, 밑바닥에 있는 이들에게 밑바닥에 있는 삶에 만족하도록 만들어서 사회적 혼란을 야기하지 않도록  만들어진 일종의 선전도구다.\n",
    "\n",
    "이러한 관점에 따르면 **잉여로 인한 손실은 불가피한 것이지만, 가능한한 손실을 최소화해야한다**.\n",
    "오늘 소개할 손실함수 관점에서의 소프트 마진 SVM은 이러한 관점과 유사하게 유도된다.\n",
    "\n",
    "\n",
    ">**주의!**\n",
    ">\n",
    "> 필자가 소프트 마진 SVM을 설명하는 두 관점을 소개하기 위해 사회적 가치에 대한 여러 메타포들을 들고왔다.\n",
    "> 사회과학에서 동일한 하나의 현상을 전혀 다른 시각에서 관찰하는 것이 빈번한데, 동일한 소프트 마진 SVM을 바라보는 두 방식을 사회현상에 빗대어 설명하는 것이 흥미로울 것이라 생각했을 뿐이다.\n",
    "> 따라서 포스팅에 포함된 사회적 가치에 대한 메타포는 설명을 위해 도입한 메타포일뿐, 필자가 사회를 비판하기 위해 들고온 개인적 사상이 아님을 밝힌다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 7.1 손실함수\n",
    "\n",
    "**손실함수**(Loss function)란 지도학습 모델이 풀고자 하는 문제에 대해 예측을 얼마나 틀렸는지를 측정하는 함수다.\n",
    "예를들어, 선형회귀 문제에서는 회귀 예측값 $i$번째 샘플에 대한 $\\hat{y}_i$과 실제 값 $y_i$와의 **평균제곱오차**(Mean Square Error)를 손실함수로 사용했다.\n",
    "$$ MSE = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat{y_i})^2$$\n",
    "손실함수 관점에서 지도학습 문제는 손실함수 값이 최소가 되도록 예측 모델을 만드는 것이 목적이다.\n",
    "\n",
    "이진분류 문제에서 사용되는 손실함수는 여러가지가 있지만, 이번 시리즈에서 다루기엔 주제를 벗어나므로 SVM에서 사용되는 손실함수에 대해서만 다루고자 한다.\n",
    "SVM 모델의 제약조건을 다시 떠올려보면, 모델이 정답을 맞췄을 때의 조건을 $y_if(x_i) \\geq 1$로 심플하게 표현하고 있다. \n",
    "그러므로 $y_if(x_i)$의 값이 1보다 작은 값이 나올수록 정답에서 더욱 멀어진다고 할 수 있다.\n",
    "따라서 이 값을 기준으로 손실함수를 정의하고 측정한다면 모델이 문제에 대해 예측을 얼마나 틀렸는지 적절히 측정할 수 있을 것이다.\n",
    "\n",
    "$t = yf(x)$라고 두자. \n",
    "$t \\geq 1 (\\iff 1-t \\leq 0)$인 경우는 정답을 맞춘 경우이므로 오차를 부여할 필요가 없다.\n",
    "한편, $0< t < 1 (\\iff 0 < 1-t < 1)$인 경우는 정답을 맞추기는 했지만 마진영역 내부로 들어와 앞으로의 데이터에 대해 의심이 되므로 $1-t$ 만큼을 오차로 부여한다.\n",
    "마지막으로 $t < 0 (\\iff 1-t \\geq 1)$인 경우는 정답을 틀린 경우이므로 $1-t$ 만큼을 오차로 부여한다.\n",
    "\n",
    "> 1. $\\;\\;\\;\\;\\;\\;t \\geq 1  \\implies h(t) = 0$ (정답을 맞춘 경우)\n",
    "> 2. $0< t < 1  \\implies h(t) = 1 - t$ (정답을 맞추었지만 의심되는 경우)\n",
    "> 3. $\\;\\;\\;\\;\\;\\;t < 0  \\implies h(t) = 1-t$ (예측이 틀린 경우)\n",
    "\n",
    "\n",
    "이를 하나의 함수로 표현하면 \n",
    "\n",
    "$$h(t) = \\max\\left\\{0, 1-t\\right\\}$$\n",
    "\n",
    "가 된다.\n",
    "이렇게 정의된 손실함수 $h(t)$를 **힌지 손실**(hinge loss)라고 부른다.\n",
    "정확히는 $t$가 예측값 $\\hat{y} = f(x)$와 실제값 $y$에 의해 정의되므로, $h$는 이들에 대한 이변수함수 $h(y,\\hat{y})$로 표기하는 것이 더 정확하다.\n",
    "\n",
    "![<img src = 'hinge_loss.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/hinge_loss.png?raw=true)\n",
    "\n",
    "> **참고!**\n",
    "> \n",
    "> 지금 논의중인 SVM은 소프트 마진 SVM이다.\n",
    "> 다시말해, 방금 정의한 힌지 손실함수는 소프트 마진 SVM에 대한 손실함수이다.\n",
    "> 하드 마진 SVM는 모델이 틀리는 것을 용납하지 못하기 때문에 정답이 아닌 것에 대해 무한한 오차를 부여한다.\n",
    "> 따라서 하드 마진 SVM에 대한 손실함수는 다음과 같이 정의된다.\n",
    "> $$h_{hard}(t) = \\begin{cases} 0 & (t\\geq 1) \\\\ \\infty & (t<1) \\end{cases}$$\n",
    "\n",
    "\n",
    "### 7.2 소프트 마진 SVM의 정의 : 손실함수 관점\n",
    "\n",
    "손실함수 관점에서의 소프트 마진 SVM은 꽤나 냉철한 경영적 마인드가 반영된다.\n",
    "경영의 마인드에서 회사의 이윤을 창출하는 가장 쉬운 방법은 혁신적인 신제품을 출시하는 것보다, 현재 회사에 불필요하게 지출되고 있는 인건비나 원자재 비용을 절감하는 것이다.\n",
    "이 관점에 따르면 어떤 문제를 해결하기 위해 각 데이터마다 이 데이터가 얼마나 손실을 야기하는지 측정하고, 이를 최소화 하는 방향을 찾는다.\n",
    "또한 문제를 해결하는데 필요한 모델에 투입되는 자원도 가능한 최소화 한다.\n",
    "\n",
    "SVM 모델에 투입되는 자원의 크기를 파라미터의 크기 $\\frac{1}{2}\\lVert \\beta \\rVert^2$으로 해석하고 데이터가 야기하는 손실의 크기를 힌지 손실함수 값 $h(y_i,\\hat{y}_i)$로 해석하면, 소프트 마진 SVM 문제는 다음의 최적화 문제로 표현될 수 있다.\n",
    "\n",
    "$$\\min_{\\beta, \\beta_0} \\frac{1}{2} \\lVert \\beta \\rVert^2 + C\\sum_{i=1}^N h(y_i,\\hat{y}_i)$$\n",
    "\n",
    "이때 첫번째 항 $\\frac{1}{2}\\lVert \\beta \\rVert^2$은 모델 파라미터의 크기를 $l_2$ norm을 사용하여 규제하도록 만들기 때문에 **$l_2$ 규제항**이라 불린다.\n",
    "그리고 두번째 항$C\\sum_{i=1}^N h(y_i,\\hat{y}_i)$은 손실함수에 의해 측정된 모델의 오차이기 때문에 **오차항**이라고 불린다.\n",
    "만약 **릿지 규제**(Ridge regularization)에 대해 들어본 독자가 있다면, 정확히 그 개념이 현재의 최적화 문제에 해당한다.\n",
    "\n",
    "[이전에](https://velog.io/@shlee0125/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%A0%95%EB%A6%AC-Support-Vector-Machine-04.-Hard-Margin-SVM-2) $\\frac{1}{2}\\lVert \\beta \\rVert^2$를 최소화하는 문제는 마진을 최대화하는 문제와 동치임을 보였었음을 상기해보자.\n",
    "여기서는 마진 최대화 문제가 모델의 파라미터에 규제를 주는 것으로 해석되고 있다.\n",
    "\n",
    "### 7.3 하나의 문제를 바라보는 두 관점\n",
    "\n",
    "지난 포스팅에서는 소프트 마진 SVM을 **잉여**의 관점에서 살펴보았고, 이번 포스팅에서는 **손실**의 관점에서 살펴보았다.\n",
    "서로 다른 두 관점에서 바라본 문제는 사실 같은 문제인걸까?\n",
    "\n",
    "잉여관점에서의 소프트 마진 SVM 문제는 제약조건이 주어진 최적화문제였다.\n",
    "\n",
    "$$\\min_{\\beta, \\beta_0, \\xi} \\frac{1}{2} \\lVert \\beta \\rVert^2 + C\\sum_{i=1}^N \\xi_n \\\\ \\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq 1 - \\xi_i, \\xi_i \\geq 0, \\forall i=1,\\ldots,N$$\n",
    "\n",
    "여기서 제약조건을 유심히 살펴보자.\n",
    "제약조건에 있는 $y_i(x_i^T\\beta + \\beta_0) \\geq 1 - \\xi_i, \\xi_i \\geq 0$라는 조건은 \n",
    "\n",
    "$$\\xi_i \\geq 1 -y_i(x_i^T\\beta + \\beta_0)=1-y_if(x_i) = 1 - y_i\\hat{y}_i$$\n",
    "\n",
    "와 \n",
    "\n",
    "$$\\xi_i \\geq 0$$\n",
    "\n",
    " 로 분리할 수 있다.\n",
    "이 두 제약조건을 동시에 만족시키는 $\\xi_i$는 \n",
    "\n",
    "$$\\xi_i = \\max\\left\\{0, 1-y_i\\hat{y}_i\\right\\}$$\n",
    "\n",
    "이다.\n",
    "\n",
    "이로부터 소프트 마진 SVM에서 제약조건을 만족시키는 잉여변수 $\\xi_i$의 값은 사실 손실함수 값 $h(y_i, \\hat{y}_i)$와 같음이 보여진다.\n",
    "이를 이용해 소프트 마진 SVM 문제에서 제약조건을 모두 소거하면 다음 제약없는 최적화 문제를 얻는다.\n",
    "\n",
    "$$\\min_{\\beta, \\beta_0} \\frac{1}{2} \\lVert \\beta \\rVert^2 + C\\sum_{i=1}^N h(y_i,\\hat{y}_i)$$\n",
    "\n",
    "따라서 잉여관점에서 유도한 소프트 마진 SVM과 손실함수 관점에서 유도한 소프트 마진 SVM은 동치다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "지금까지 소프트 마진 SVM에 대해 알아보았다.\n",
    "\n",
    "그러나 여전히 다 맺지 못한 이야기들이 많다.\n",
    "\n",
    "수학적으로 소프트 마진 SVM 문제의 최적해를 어떻게 구하는지에 대한 논의와 실용적으로 페널티 파라미터 $C$가 모델에 어떤 영향을 끼치는지에 대해서는 추후에 다루기로 하자.\n",
    "\n",
    "다음 포스팅에서는 SVM 이론의 *치트키*라고 할 수 있는 **커널 트릭**(Kernel trick)에 대해서 다뤄보겠다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "# Implement Hinge loss function from scratch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "colors = [plt.cm.Dark2(i) for i in range(20)]\n",
    "\n",
    "\n",
    "\n",
    "t = np.linspace(0,2,100)\n",
    "\n",
    "def h(t):\n",
    "    error = 1-t\n",
    "    zeros = np.zeros_like(t)\n",
    "    return np.max([zeros, error], axis = 0)\n",
    "\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.plot(t, h(t), color = colors[0], label = '$h(x) = \\max(0,1-t)$')\n",
    "plt.title('Hinge Loss', fontsize =20)\n",
    "plt.xlabel('$t = yf(x)$', fontsize = 14)\n",
    "plt.ylabel('$h(t)$', fontsize = 14)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.savefig('hinge_loss.png')\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implement Hinge loss function from scratch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "colors = [plt.cm.Dark2(i) for i in range(20)]\n",
    "\n",
    "\n",
    "\n",
    "t = np.linspace(0,2,100)\n",
    "\n",
    "def h(t):\n",
    "    error = 1-t\n",
    "    zeros = np.zeros_like(t)\n",
    "    return np.max([zeros, error], axis = 0)\n",
    "\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.plot(t, h(t), color = colors[0], label = '$h(x) = \\max(0,1-t)$')\n",
    "plt.title('Hinge Loss', fontsize =20)\n",
    "plt.xlabel('$t = yf(x)$', fontsize = 14)\n",
    "plt.ylabel('$h(t)$', fontsize = 14)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.savefig('hinge_loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 커널 트릭\n",
    "\n",
    "\n",
    "\n",
    "### 8.1 여전히 남은 문제... 비선형 분류.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5cb7460d01971850367bd1b2daf3e8adb7995025ce41fb9ea96ac3b069967568"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dacon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
