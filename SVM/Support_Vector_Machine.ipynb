{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 포스팅 시리즈는 다양한 머신러닝 테크닉에 대해 수학적 관점과 실용적 관점에서 정리한다.\n",
    "\n",
    "필자는 수학을 전공했기 때문에 수학적 접근과 용어에 대해 익숙하게 사용한 것이 있지만, 수학을 전공하지 않은 사람들에겐 다소 낯선 접근과 용어가 있을 수 있다.\n",
    "\n",
    "최대한 그러한 부분을 자세히 설명하려 노력하였지만 필자의 타전공자에 대한 '공감능력부족'으로 효과적으로 전달되지 못한 부분이 있을 것으로 생각된다.\n",
    "\n",
    "이 글을 읽어주시는 분께 일차적으로 감사드리며, 해당 부분에 대해 질문이나 코멘트를 남겨주시는 분께는 거듭제곱으로 감사드림을 말씀드린다.\n",
    "\n",
    "# Support Vector Machine\n",
    "\n",
    "서포트 벡터 머신은 딥러닝이 등장하기 이전에 가장 유명하고 성능 좋은 머신러닝 모델이었다고 한다. \n",
    "현재는 다소 실무에서 사용되는 정도가 줄어들었겠지만, 서포트 벡터 머신에 적용되는 다양한 수학적 테크닉들은 인공지능을 이해하고 연구하는 데에 여전히 훌륭한 인사이트를 준다고 생각한다.\n",
    "특히 서포트 벡터 머신의 아이디어는 유클리드 기하학과 최적화 이론으로 설명이 된다는 점은 수학자들에게 있어서 굉장히 매력적이다.\n",
    "\n",
    "본 포스팅의 내용은 다음의 자료들을 참고했다.\n",
    "\n",
    "> 1. Mathematics for Machine Learning (Deisenroth, Marc Peter and Faisal, A. Aldo and Ong, Cheng Soon)\n",
    "> 2. The Elements for Statistical Learning (Trevor Hastie, Robert Tibshirani, Jerome Friedman)\n",
    "> 3. 김민준님(이화여자대학교, 수학과 석사)의 SVM Lecture note\n",
    "> 4. 김원화 교수님(포항공과대학교, 인공지능대학원 교수)의 데이터 마이닝 Lecture note\n",
    "> 5. Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow (Aurelien, Geron)\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "**Support Vector Machine**(SVM)은 데이터를 분류하기 위해 클래스 사이의 마진(margin)을 최대화하는 초평면을 찾는다. \n",
    "\n",
    "예를들어, 두 개의 특성변수를 가지는 이진분류 문제는 데이터를 2차원 평면상에 표현할 수 있으며, SVM은 이들 두 클래스를 분할하면서 마진(직선 양 옆으로 평행하게 뻗은 띠)을 최대화하는 직선을 찾는다.\n",
    "\n",
    "> **용어정리**\n",
    "> \n",
    "> - *서포트 벡터 (Support Vector)* : 새로운 데이터가 들어왔을 때, 해당 데이터를 구분시켜줄 기준이 되는 샘플\n",
    "> - *결정 경계 (Decision Boundary)* : 모델에 의해 만들어진 클래스를 구분할 기준이 되는 초평면\n",
    "> - *마진 (Margin)* : 결정 경계를 중심으로 서포트 벡터까지의 거리만큼 나란하게 떨어진 영역\n",
    "\n",
    "![svm_figure.png](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/margin.png?raw=true)\n",
    "\n",
    "SVM은 확률모델에 기반으로한 머신러닝 모델들과는 다른 두 가지 특징을 가진다.\n",
    "\n",
    "> 1. SVM은 지도학습 문제를 기하학적인 방법으로 접근한다. 이는 확률모델의 관점으로 접근하는 많은 머신러닝 모델들과 구별되는 점이다.\n",
    "> 2. (soft-margin) SVM의 최적화 문제는 해석적 해를 구하기 어렵다. 따라서 다양한 최적화 방법론(e.g. 경사하강법)에 의존해야 한다.\n",
    "\n",
    "확률모델에 기반한 머신러닝 모델은 *최대우도추정*(Maximum Likelihood Estimation)과 *베이지안 추정*(Bayesian inference)를 취하고 있다.\n",
    "이러한 관점에서 확률기반 모델은 데이터의 유사성에 대한 확률론적 시각(e.g. 확률분포)을 기반으로한 최대우도를 추정하는 최적화문제로 모델추론에 접근한다.\n",
    "\n",
    "그러나 SVM은 데이터의 유사성에 대한 기하학적 시각을 기반으로 최적화문제를 제시한다.\n",
    "데이터의 유사성에 대한 기하학적 시각은 *내적*(inner product)과 *거리*(metric) 개념에 의존한다.\n",
    "SVM은 두 클래스 간의 거리(margin)를 가장 크게 만드는 최적화문제로 모델추론에 접근한다.\n",
    "\n",
    "SVM은 훌륭한 모델이지만, 현재는 딥러닝 모델이 훨씬 범용적으로 사용되고 좋은 성능을 보여준다.\n",
    "이러한 점에서 필자는 단순히 SVM을 얕게 이해하고 모델을 구현하는 방법을 익히는 것이 그렇게 큰 도움이 될 것이라고 생각하지 않는다.\n",
    "그러나 SVM을 공부하는 것은 인공지능 모델을 설계하는 데에 다양한 기하학적 직관을 제공한다는 점에서 공부할 가치가 있다고 생각한다.\n",
    "SVM 이론에 포함된 기하학적 인사이트, 선형분류의 한계를 뛰어넘기 위한 다양한 전략들은 수학적으로 잘 formulation 되어 있기 때문에 수학적으로 훈련된 이들에겐 SVM만큼 다양한 인사이트를 제공하는 모델이 없다고 생각한다.\n",
    "본 포스팅을 작성하는 필자나 읽는 독자나 이 시리즈가 마쳤을 때에 자신만의 훌륭한 인사이트를 가지고 떠나길 바라는 바이다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Binary Classification\n",
    "\n",
    "이진분류 문제는 데이터마다 할당된 출력변수 $y$의 값이 두 가지인 경우를 말한다.\n",
    "\n",
    "예를들어, 소비자들의 다양한 정보를 입력받아 이 소비자가 특정 상품을 구매 할지 안할지 예측하는 문제나, 신호에 대한 정보를 입력받아 이 정보가 진짜 신호인지 가짜 신호인지 분별하는 문제가 이진분류 문제이다.\n",
    "\n",
    "이진분류 문제에서 출력변수는 0 또는 1로 표기하거나 -1 또는 +1로 표기한다.\n",
    "Support Vector Machine을 설명할 때엔 이진분류 문제의 출력변수를 주로 -1과 +1로 표현한다.\n",
    "\n",
    "이진분류 문제를 수학적으로 표현해보자.\n",
    "\n",
    "$N$개의 데이터 $(x_1,y_1), (x_2,y_2), \\ldots , (x_N, y_N)$으로 구성된 훈련 데이터가 주어졌다고하자.\n",
    "여기서 $x_i \\in \\mathbb{R}^d$ 이고, $y \\in \\left\\{-1, +1\\right\\}$이다. \n",
    "\n",
    "지도학습의 관점에서 이진분류 문제는 훈련 데이터를 학습하여 분류 오차를 최소가 되게 하는 분류함수 $G : \\mathbb{R}^d \\rightarrow \\left\\{-1, +1 \\right\\}$를 추정하는 문제이다.\n",
    "\n",
    "선형분류 문제는 훈련 데이터로부터 적절한 선형함수 $f(x) = x^T \\beta + \\beta_0$를 적합하여, 이로부터 정의된 초평면 $\\left\\{ x \\in \\mathbb{R}^d : f(x) = x^T \\beta + \\beta_0 = 0 \\right\\}$을 찾는 문제다.\n",
    "여기서 $\\beta_0 \\in \\mathbb{R}$은 선형함수 $f$의 *절편*(intercept)이며, 파라미터 $\\beta \\in \\mathbb{R}^d$ ($\\lVert \\beta \\rVert = 1$)는 초평면에 수직인 단위벡터, 즉 *법선벡터*(normal vector)이다. \n",
    "\n",
    "분류함수 $G$는 $f(x)$에 의해 $$G(x) = sign[f(x)] = sign[x^T\\beta + \\beta_0]$$ 로 유도된다.\n",
    "여기서 $sign(x) = \\begin{cases} +1 & (x \\geq 0) \\\\ -1 & (x < 0)\\end{cases}$는 *부호함수*이다.\n",
    "그리고 초평면 $\\left\\{ x \\in \\mathbb{R}^d : f(x) = x^T \\beta + \\beta_0 = 0 \\right\\}$를 **결정경계**(Decision boundary)라고 부른다.\n",
    "\n",
    "선형함수 $f(x)$의 파라미터 $\\beta$는 결정경계를 기준으로 방향을 결정한다.\n",
    "테스트 데이터 $x_{test}$가 $f(x_{test}) \\geq 0$이면 결정경계의 *양의 방향*에 놓인다고 하며, 이 경우 $G(x_{test}) = +1$이다.\n",
    "유사하게, $f(x_{test}) < 0$이면 데이터가 결정경계의 *음의 방향*에 놓인다고 하며, 이 경우 $G(x_{test}) = -1$이다.\n",
    "\n",
    "훈련 데이터 $x_i$가 $y_i = +1$의 레이블을 가진다면, $f(x_i) \\geq 0$이 되어야 $G(x_i) = +1$이 된다.\n",
    "마찬가지로 $y_i = -1$의 레이블을 가진다면, $f(x_i) < 0$이 되어야 $G(x_i) = -1$이 되어 정확한 예측이 된다.\n",
    "**선형분류 문제**는 모든 $i$에 대하여 $y_if(x_i) > 0$이 되게 하는 선형함수 $f(x) = x^T \\beta + \\beta_0$를 찾을 수 있는 문제를 말한다.\n",
    "\n",
    "![linear_classification.png](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/linear_classification.png?raw=true)\n",
    " \n",
    "이때 두 클래스를 분류할 수 있는 선형함수 $f(x)$의 선택, 즉 결정경계는 여러개 존재할 수 있다.\n",
    "그렇다면 어떤 결정경계를 선택하는 것이 가장 일반화 성능이 좋다고 할 수 있을까?\n",
    "\n",
    "SVM은 이 질문에 대해 유클리드 기하학과 최적화 이론의 언어로 답을 한다.\n",
    "모델의 융통성을 얻기 위해 SVM은 **마진**(margin)이 최대가 되도록 선형함수 $f(x)$를 선택한다.\n",
    "마진은 유클리드 기하학의 개념으로 결정경계의 근방영역을 정의하며, 이 영역이 넓을 수록 결정경계 근방에 위치하게 될 테스트 데이터에 대해 유연하게 대처할 수 있게 된다.\n",
    "이러한 마진 최대화 문제는 수학적으로 최적화 이론의 언어로 잘 정의되며, 이를 알고리즘으로 구할 수 있다.\n",
    "\n",
    "다음 포스팅부터는 이러한 개념의 정의에 대해 차근차근 알아가보도록 하겠다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **연습문제 1)**\n",
    "> 벡터 $\\beta$가 초평면 $\\left\\{x \\in \\mathbb{R}^d : f(x) = x^T \\beta + \\beta_0 = 0\\right\\}$와 수직임을 보여라.\n",
    "> \n",
    "> *Solution)*\n",
    "> \n",
    "> 초평면 위의 임의의 두 점 $x_a$, $x_b$에 대하여, 초평면 위의 위치벡터 $x_a - x_b$를 생각하자. \n",
    "> \n",
    "> 이때 초평면의 정의에 의해 $f(x_a) = 0, f(x_b) = 0$이이다.\n",
    "> \n",
    "> 한편, $f(x_a) - f(x_b) = (x_a^T\\beta + \\beta_0) - (x_b^T\\beta + \\beta_0) = (x_a - x_b)^T\\beta$가 성립하므로 $(x_a - x_b)^T\\beta = 0$임을 얻는다. \n",
    "> \n",
    "> 따라서 $\\beta$는 초평면 위의 임의의 위치벡터에 대하여 수직이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hard Margin SVM (1)\n",
    "\n",
    "**마진**(margin)은 결정경계와 그것과 가장 가까이 있는 훈련 데이터와의 거리를 말한다.\n",
    "마진은 학창시절에 배운 *점과 직선 사이의 거리공식*을 일반화한 개념으로 계산한다.\n",
    "\n",
    "### 3.1 점과 직선 사이의 거리공식\n",
    "먼저 점과 직선 사이의 거리 공식을 복습해보자.\n",
    "중고등학교 시절 점 $(x_0,y_0)$와 직선 $ax+by+c = 0$ 사이의 거리 $d$는 아래의 공식으로 구했었다.\n",
    "$$d = \\frac{ax_0 + by_0 + c}{\\sqrt{a^2 + b^2}} $$\n",
    "이 공식은 보통 고등학교 1학년 과정에서 배우게 되는데 고등학교 1학년때는 벡터의 개념을 배우지 않기 때문에 그 유도과정이 상당히 작위적이다. \n",
    "여기서는 벡터를 이용해 위 공식을 다시 유도해보고자 한다.\n",
    "\n",
    "먼저 직선이 어떻게 정의되는지 살펴보자.\n",
    "직선의 방정식이라고 말하면 보통 일차함수 $y = ax + b$ 꼴을 생각한다. \n",
    "이 일차함수에서 $x$의 계수 $a$는 직선의 기울기를, $b$는 $y$-절편을 의미했다.\n",
    "\n",
    "그런데 유달리 점과 직선 사이의 거리 공식을 사용할 땐, 직선을 일차방정식 $ax + by + c = 0$의 꼴로 정의했다.\n",
    "아직 내적의 개념을 배우지 않은 고등학교 1학년에게 이러한 형태의 직선의 방정식에서 $x$와 $y$ 그리고 상수항이 가진 계수 $a,b,c$의 의미를 해석하기가 어려웠다.\n",
    "이 포스팅을 읽고있는 독자들 중에서도 혹시 이 공식의 해석에 대해 잘 몰랐던 분이 계시다면, 이 기회에 배워가보시길 바란다.\n",
    "\n",
    "<img src = 'distance.png'>\n",
    "\n",
    "\n",
    "먼저 원점을 지나는 직선 $ax + by = 0$을 생각해보자.\n",
    "이 직선 위의 점은 방정식 $ax + by = 0$을 만족시키는 벡터 $[x,y]^T \\in \\mathbb{R}^2$들의 집합으로 표현된다.\n",
    "그런데 $x,y$를 벡터로 본 순간, 식 $ax + by$는 벡터의 내적 공식으로 보인다!\n",
    "즉, 방정식 $ax + by = 0$은 사실 **두 벡터 $[a,b]^T \\in \\mathbb{R}^2$과 $[x,y]^T \\in \\mathbb{R}^2$의 내적 $\\langle [a,b]^T, [x,y]^T \\rangle = [a,b]\\begin{bmatrix} x \\\\ y \\end{bmatrix}$의 값이 0이 된다**는 의미다.\n",
    "내적이 0이 된다는 의미는 **두 벡터가 수직**임을 의미한다.\n",
    "따라서 직선 $ax+by=0$은 벡터 $[a,b]^T$와 수직인 벡터들의 자취를 의미하게 된다.\n",
    "이런 점에서 직선의 방정식 $ax + by = 0$의 계수벡터 $[a,b]^T$는 직선의 진행방향에 수직인 방향을 나타내고, 이를 **법선벡터**(normal vector)라고 부른다.\n",
    "\n",
    "이제 좌표공간 위의 점 $(x_0, y_0)$와 원점을 지나는 직선 $ax + by = 0$ 사이의 거리를 구해보자.\n",
    "점 $(x,y)$를 점 $(x_0, y_0)$의 직선 $ax + by = 0$ 위로의 정사영이라고 하자. \n",
    "그런데 앞에서 법선벡터 $[a,b]^T$는 직선 $ax+by = 0$과 수직이라고 했으므로, $[a,b]^T$와 $[x_0 - x, y_0 - y]^T$는 평행하다.\n",
    "즉, 어떤 상수 $\\alpha \\in \\mathbb{R}$가 존재하여, \n",
    "$$[x_0 - x, y_0 - y]^T = \\alpha [a,b]^T$$\n",
    "가 성립한다.\n",
    "이때 법선벡터 $[a,b]^T$의 크기를 1로 정규화해 방향에 대한 정보만 남겨주면, $(x_0,y_0)$는 $(x,y)$를 $\\frac{1}{\\sqrt{a^2 + b^2}}\\begin{bmatrix} a \\\\ b \\end{bmatrix}$ 방향으로 거리 $d$ 만큼 평행이동한 것이므로, \n",
    "$$\\alpha = \\frac{d}{\\sqrt{a^2 + b^2}}$$\n",
    "이고\n",
    "$$ \\begin{bmatrix} x_0-x \\\\ y_0-y \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\frac{d}{\\sqrt{a^2 + b^2}}\\begin{bmatrix} a \\\\ b \\end{bmatrix}$$\n",
    "임을 유도할 수 있다.\n",
    "\n",
    "그런데 $(x_0,y_0)$와 $(x,y)$ 사이의 거리 $d$는 \n",
    "$$d = \\sqrt{[x_0-x,y_0-y]\\begin{bmatrix} x_0-x \\\\ y_0-y \\end{bmatrix}}$$ \n",
    "이 성립하므로\n",
    "$$ d^2 = [x_0-x,y_0-y]\\begin{bmatrix} x_0-x \\\\ y_0-y \\end{bmatrix} = [x_0-x,y_0-y]\\left(\\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\frac{d}{\\sqrt{a^2 + b^2}}\\begin{bmatrix} a \\\\ b \\end{bmatrix}\\right)$$\n",
    "가 성립한다.\n",
    "\n",
    "이때 $[x_0-x,y_0-y]\\begin{bmatrix} x \\\\ y \\end{bmatrix} = 0$이고 $ax + by =0$ 이므로 위 식을 정리하면, \n",
    "$$ d^2 = \\frac{d}{\\sqrt{a^2 + b^2}}\\begin{bmatrix} x_0-x \\\\ y_0-y \\end{bmatrix}^T\\begin{bmatrix} a \\\\ b \\end{bmatrix} = d\\frac{ax_0 + by_0}{\\sqrt{a^2 + b^2}}$$\n",
    "이 되어, \n",
    "$$d = \\frac{ax_0 + by_0}{\\sqrt{a^2 + b^2}}$$\n",
    "임을 얻는다.\n",
    "\n",
    "중간에 몇 가지 계산과정을 생략했는데 손으로 계산해보면서 생략된 부분을 직접 매꾸어 보면 이 유도과정을 이해하는 데에 도움이 될 것이다.\n",
    "\n",
    "(원점을 지나지 않는 직선의 케이스는 평행이동의 개념으로 옮겨서 이해한다. 추후 추가 예정)\n",
    "\n",
    "\n",
    "### 3.2 마진(margin)\n",
    "이제 마진을 정의하기 위해 필요한 준비는 모두 끝났다.\n",
    "$x_a \\in \\mathbb{R}^d$가 결정경계와 가장 가까운 훈련 데이터라고 하고, 결정경계는 선형함수 $f(x) = x^T\\beta + \\beta_0$에 의해 정의된다고 하자.\n",
    "이때 선형함수의 법선벡터 $\\beta$에 대하여, $\\lVert \\beta \\rVert =  1$이라 가정한다.\n",
    "$x_a$와 결정경계 사이의 최단거리는 $x_a$의 결정경계 위로의 사영 $x_a'$와 $x_a$ 사이의 거리 $d$로 정의된다.\n",
    "그런데 결정경계와 단위법선벡터 $\\beta$가 수직이므로, 벡터 $x_a$는 $x_a' + d\\beta$로 표현될 수 있다.\n",
    "\n",
    "\n",
    "![<img src = 'margin.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/margin.png?raw=true)\n",
    "\n",
    "위와 같은 개념을 생각하면, 결정경계의 양의 방향과 음의 방향으로 $d$ 만큼 떨어진 영역을 생각할 수 있다.\n",
    "이 영역 또는 이 영역의 폭을 **마진**(margin)이라고 부른다.\n",
    "마진을 이용하면 모든 $i$에 대하여, $y_i f(x_i) \\geq d$가 되도록 하는 선형함수 $f(x)$를 찾는 문제로 선형분류 문제를 변형할 수 있다.\n",
    "\n",
    "\n",
    "![<img src = 'margin_ex.png'>](https://github.com/SHlee-TDA/ML_education/blob/main/SVM/margin_ex.png?raw=true)\n",
    "\n",
    "\n",
    "마진을 최대화 하는 선형함수 $f(x)$를 찾는 문제는 다음의 최적화문제로 표현할 수 있다.\n",
    "$$ \\max_{\\beta, \\beta_0, d} d \\\\ \\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq d, d>0,  \\forall i=1,2,\\ldots, N$$\n",
    "SVM은 위 최적화문제의 해로서 결정경계를 선택한다.\n",
    "즉, 마진의 경계 외부에서 클래스가 분류되도록 하면서 마진의 크기를 최대화 시키는 문제다.\n",
    "\n",
    "마진 최대화 문제를 이렇게 해석하면 직관적으로 문제를 이해하기가 편리하다.\n",
    "그러나 이 최적화 문제를 푸는 데에는 다소 유리하지 않다.\n",
    "\n",
    "다음 포스팅에서는 이 최적화 문제를 수학적으로 풀기 쉬운 형태로 유도하는 방법에 대해서 다루도록 하겠다.\n",
    "\n",
    "\n",
    "> **주의!**\n",
    ">\n",
    "> 마진의 개념은 유클리드 공간 상의 거리 개념으로부터 유도된다.\n",
    "> 그러나 거리 개념은 데이터의 스케일에 따라 혼란을 야기할 수 있다.\n",
    ">\n",
    ">예를들어, 입력변수 $x_i$가 '계좌잔액', '개설연도'로 구성된 데이터라고 할 때 $x_a = (1000000, 2020)$, $x_b = (10000, 2020)$, $x_c = (1000020, 2000)$ 세 가지 샘플의 거리는 어떻게 이해할 것인가?\n",
    ">$x_a$와 $x_b$는 개설연도가 같지만, 계좌잔고가 100배나 차이나므로 거리가 굉장히 크다.\n",
    ">$x_a$와 $x_c$는 계좌잔고가 20원밖에 차이가 나지 않지만, 개설연도가 20년이나 차이가 난다.\n",
    ">$x_b$와 $x_c$ 중에 $x_a$와 더 유사한 데이터는 어느 것인가?\n",
    ">이는 문제에 따라 다를 것이다. \n",
    ">그러나 단순히 Euclidean 거리로 계산했을 때엔 $x_a$와 $x_b$ 사이의 거리가 $x_a$와 $x_c$ 사이의 거리보다 매우 크다.\n",
    ">\n",
    ">이러한 점 때문에 거리 개념을 기반으로한 SVM 모델은 데이터의 scale에 민감하다고 할 수 있다.\n",
    ">따라서 SVM 모델을 학습하기 전에 스케일링을 위한 전처리기법(e.g. Standard Scaling)을 수행하는 것이 일반적이다.\n",
    ">데이터의 스케일링에 대하여 더 자세한 논의는 후에 하도록 하고 지금은 데이터의 스케일링 이슈는 고려하지 않도록 하자.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hard Margin SVM (2)\n",
    "\n",
    "\n",
    "앞에서는 선형함수 $f(x)$의 방향만 고려하기 위해 제약조건으로 $\\lVert \\beta \\rVert = 1$이 되게 두고 마진 최대화 문제를 유도했다.\n",
    "만약 선형함수 $f(x)$의 법선벡터 $\\beta$가 단위벡터가 아니더라도, 그 크기로 정규화 $(\\beta / \\lVert \\beta \\rVert)$해주는 것으로 동일한 문제를 유도할 수 있다.\n",
    "\n",
    "이번에는 파라미터 벡터 $\\beta$를 정규화하는것 대신 단위 또는 기준이 될 데이터를 선택하는 것으로 동일한 문제를 유도해보겠다.\n",
    "우리가 선텍할 기준은 예측값 $f(x) = x^T \\beta + b$의 값이 1이 되는 데이터를 '가장 가까운 데이터'라고 생각하는 것이다.\n",
    "이러한 데이터를 앞에서와 마찬가지로 $x_a$로 표기하자.\n",
    "앞에서는 $x_a$에 대해 특별한 가정 없이 '가장 가까운 데이터'로 선택했지만, 이번에는 '가장 가깝다'의 기준을 선형함수 $f(x)$의 값을 기준으로 설정한다는 점에서 차이가 있음에 유의하자.\n",
    "\n",
    "그러면 $x_a$는 $f(x_a) = 1$을 만족시키므로, 초평면 $\\left\\{x \\in \\mathbb{R}^d : f(x) = 1 \\right\\}$ 위에 놓이게 된다.\n",
    "이번에도 마찬가지로 $x_a$의 결정경계 위로의 정사영 $x_a'$를 생각한다.\n",
    "그러면 $x_a'$는 방정식 \n",
    "$$f(x_a') = x_a'^T \\beta + \\beta_0 =  0 \\;\\; \\cdots (1)$$\n",
    "을 만족시킨다.\n",
    "이때 $\\beta$가 앞에서와 달리 단위벡터일 필요가 없음에 유의하자.\n",
    "한편, 결정경계와 $x_a$ 사이의 거리를 $d$라고 두면 아래의 관계식이 성립한다.\n",
    "$$ x_a' = x_a - d\\frac{\\beta}{\\lVert \\beta \\rVert} \\;\\; \\cdots (2)$$\n",
    "이제 이 식을 위 방정식 (1)에 대입하여 정리하면, 다음 관계식을 얻는다. (유도과정은 연습문제로 남겨둔다.)\n",
    "$$ d = \\frac{1}{\\lVert \\beta \\rVert} \\cdots (3)$$\n",
    "\n",
    "<img src = 'margin2.png'>\n",
    "\n",
    "$d$를 이러한 방식으로 유도해주면 모든 $i$에 대하여, $y_i f(x_i) \\geq 1$가 되도록 하는 선형함수 $f(x)$를 찾는 문제로 선형분류 문제를 변형할 수 있다.\n",
    "그러므로 마진 최대화 문제는 다음의 최적화 문제로 정의될 수 있다.\n",
    "$$ \\max_{\\beta, \\beta_0} \\frac{1}{\\lVert \\beta \\rVert} \\\\ \\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq 1,  \\forall i=1,2,\\ldots, N$$\n",
    "\n",
    "이 문제를 계산의 편의를 위해 $\\frac{1}{2} \\lVert \\beta \\rVert^2$를 최소화 하는 문제로 변형하자.\n",
    "이렇게 변형해도 최적해는 변함이 없지만, 해를 구하는 계산과정에서 미분 등의 연산을 할 때 훨씬 편리해진다.\n",
    "이를 통해 최종적으로 다음의 최적화 문제를 얻는다.\n",
    "$$ \\min_{\\beta, \\beta_0} \\frac{1}{2}\\lVert \\beta \\rVert^2 \\\\ \\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq 1,  \\forall i=1,2,\\ldots, N$$\n",
    "이 최적화 문제와 1.2에서 처음 정의한 마진 최대화 문제가 동치임을 보이는 것은 연습문제로 남겨둔다.\n",
    "이 문제는 선형 부등식 제약조건을 가지는 컨벡스 최적화 문제로, 라그랑주 승수법을 이용해 해석적으로 풀 수 있다.\n",
    "이 문제의 해석적 해법에 대해서는 다음에 살펴보도록 하자.\n",
    "\n",
    "위 최적화 문제는 **하드 마진 SVM**(Hard margin SVM)이라 부른다.\n",
    "이 문제를 '하드'하다고 부르는 이유는 이 문제의 유도에서 마진 조건 $y_i(x_i^T\\beta + \\beta_0) \\geq 1$에 조금의 오차도 허용하지 않기 때문이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **연습문제 2)**\n",
    "> 방정식 (1)에 방정식 (2)를 대입하여 방정식 (3)을 유도하시오.\n",
    "> \n",
    "> *Solution)*\n",
    ">\n",
    "> 방정식 (1)에 방정식 (2)를 대입하면 다음을 얻는다. \n",
    "> $$ \\left(x_a - d\\frac{\\beta}{\\lVert \\beta \\rVert}\\right)^T \\beta + \\beta_0 = 0 $$\n",
    "> 그러면 전치연산자 $T$의 성질에 의해, \n",
    "> $$ \\left(x_a^T - d\\frac{\\beta^T}{\\lVert \\beta \\rVert}\\right) \\beta + \\beta_0 = 0 $$\n",
    "> 임을 얻는다.\n",
    "> 그러면 행렬곱의 분배법칙에 의해\n",
    "> $$ \\left(x_a^T \\beta - d \\frac{\\beta^T\\beta}{\\lVert\\beta \\rVert}\\right) + \\beta_0 = 0$$\n",
    "> 이 성립한다.\n",
    "> 이제 위 식을 $$(x_a^T\\beta + \\beta_0) - d\\frac{\\beta^T\\beta}{\\lVert\\beta \\rVert} = 0$$\n",
    "> 으로 정리하자.\n",
    ">\n",
    "> 여기서 $x_a$는 $f(x_a) = 1$이 만족하도록 선택했으므로, $x_a^T\\beta + \\beta_0 = 1$이고 내적의 성질에 의해 $\\beta^T \\beta = \\lVert \\beta \\rVert^2$ 임을 이용하면,\n",
    "> $$ 1 - d\\lVert \\beta \\rVert = 0$$\n",
    "> 임을 얻는다.\n",
    "> 따라서 이를 정리하면, \n",
    "> $$d = \\frac{1}{\\lVert \\beta \\rVert} \\;\\; \\cdots (3)$$\n",
    "> 를 얻는다.\n",
    "\n",
    "---\n",
    "\n",
    "> **연습문제 3)**\n",
    "> 마진 최대화 문제 \n",
    "> $$ \\max_{\\beta, \\beta_0, d} d \\\\ \\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq d, d>0,  \\forall i=1,2,\\ldots, N$$\n",
    "> 와\n",
    "> 파라미터 최소화 문제 \n",
    "> $$ \\min_{\\beta, \\beta_0} \\frac{1}{2}\\lVert \\beta \\rVert^2 \\\\ \\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq 1,  \\forall i=1,2,\\ldots, N$$\n",
    "> 가 동치임을 보이시오.\n",
    "> \n",
    "> \n",
    "> *Solution)*\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Why do we maximize margin?\n",
    "\n",
    "그렇다면 마진을 최대화 하면 좋은 점이 무엇일까?\n",
    "아래의 그림에 넓은 마진(초록색)과 좁은 마진(노란색)의 경우를 비교해보자.\n",
    "\n",
    "<img src = 'margin_comparision.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
